---
title: 'Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning'

authors:
  - admin
  - Kaiwen Wang
  - Rahul Kidambi
  - Ryan Sullivan
  - Alekh Agarwal
  - Christoph Dann
  - Andrea Michi
  - Marco Gelmi
  - Yunxuan Li
  - Raghav Gupta
  - Kumar Dubey

# Author notes (optional)
author_notes: []

date: '2024-06-22T00:00:00Z'
doi: ''

publishDate: '2024-06-22T00:00:00Z'

publication_types: ['1']

publication: In *Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing*
publication_short: In *EMNLP 2024*

abstract: Reward-based finetuning is crucial for aligning language policies with intended behaviors (e.g., creativity and safety). A key challenge is to develop steerable language models that trade-off multiple (conflicting) objectives in a flexible and efficient manner. This paper presents Conditional Language Policy (CLP), a general framework for finetuning language models on multiple objectives. Building on techniques from multi-task training and parameter-efficient finetuning, CLP learns steerable models that effectively trade-off conflicting objectives at inference time. Notably, this does not require training or maintaining multiple models to achieve different trade-offs between the objectives. Through extensive experiments and ablations on two summarization datasets, we show that CLP learns steerable language models that outperform and Pareto-dominate existing approaches for multi-objective finetuning.

summary: We propose Conditional Language Policy (CLP), a framework for finetuning steerable language models that effectively balance multiple conflicting objectives without maintaining separate models.

tags: []

featured: true

url_pdf: 'https://arxiv.org/pdf/2407.15762'
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

image:
  caption: ''
  focal_point: 'Top'
  preview_only: false

projects: []

slides: ""
---

