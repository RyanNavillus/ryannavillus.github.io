---
title: 'Robust Multi-Objective Preference Alignment with Online DPO'

authors:
  - admin
  - Raghav Gupta
  - Ryan Sullivan
  - Yunxuan Li
  - Samrat Phatale
  - Abhinav Rastogi

# Author notes (optional)
author_notes: []

date: '2025-02-25T00:00:00Z'
doi: ''

publishDate: '2025-02-25T00:00:00Z'

publication_types: ['1']

publication: In *Proceedings of the 39th AAAI Conference on Artificial Intelligence (AAAI 2025)*
publication_short: In *AAAI 2025*

abstract: Multi-objective preference alignment of large language models (LLMs) is critical for developing AI systems that are more configurable, personalizable, helpful, and safe. However, optimizing model outputs to satisfy diverse objectives with variable weights at inference time for truly personalized models presents a significant challenge. Existing approaches are either computationally expensive to train or do not sufficiently steer model behaviors. This paper introduces the Multi-Objective Online DPO (MO-ODPO) algorithm, designed to robustly and efficiently align model behaviors with multiple, potentially conflicting human preferences. Our approach incorporates a prompt conditioning mechanism, allowing us to train a single preference-conditional policy that can adapt to new preference combinations at inference. Experiments on two popular benchmarks show that MO-ODPO Pareto-dominates existing baselines while providing excellent inference-time steerability between diverse objectives.

summary: We introduce MO-ODPO, an efficient and robust algorithm for aligning large language models with multiple conflicting preferences, allowing flexible steerability at inference.

tags: []

featured: true

url_pdf: 'https://arxiv.org/pdf/2503.00295v1'
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

image:
  caption: ''
  focal_point: 'Top'
  preview_only: false

projects: []

slides: ""
---

